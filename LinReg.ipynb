{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c512fabe-0e9e-484a-ae4f-8c7b7aefa166",
   "metadata": {},
   "source": [
    "# Least Squares Methode zur Regression\n",
    "## Lineare Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bcd9e5-5818-4d53-b1f0-8c81ba83cfc6",
   "metadata": {},
   "source": [
    "Wir wollen eine Funktion der Form $y=m \\cdot x + b + \\epsilon$ fitten. Wir besitzen einen Datensatz, und wollen nun einen linearen Fit finden.<br>\n",
    "Im 1D-Fall setzt sich unsere Funktion aus einer Matrixmultiplikation zwischen dem Gewicht und der Variablen zusammen. Hierbei steht dann:\n",
    "\\begin{equation}\n",
    "    y = w_0 + w_1 \\cdot x = \\mathbf{x}^T \\cdot \\mathbf{w} \n",
    "\\end{equation}\n",
    "mit $\\mathbf{x}=\\begin{pmatrix}\n",
    "    1\\\\\n",
    "    x\n",
    "    \\end{pmatrix}$\n",
    "und $\\mathbf{w}=\\begin{pmatrix}\n",
    "    w_0\\\\\n",
    "    w_1\n",
    "    \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e61468c-3395-40d6-a8dd-07501e870b00",
   "metadata": {},
   "source": [
    "## Least Squares\n",
    "\n",
    "Wir suchen den Fehler, den wir mit der Regression machen und wollen diesen Minimieren. Um den Fehler zu quantisieren verwendet man sog. \"Loss Funktionen\". Die Standard Loss Funktion für Regression ist der Summed Squared Error $SSE = \\sum_{i=1}^N (y_i - f(\\mathbf{x}_i))^2$. Wobei die $\\mathbf{x}_i$ unsere Datenpunkte sind und $y_i$ unser Tatsächlicher Wert. SSE ist praktisch, da die Funktion zum Einen konvex ist (d.h. nur ein Minimum) und zum Anderen differenzierbar ist.<br><br>\n",
    "\n",
    "Es wird nun so optimiert, dass die Gewichte $w$ genau SSE minimieren. Dann hat man ein Modell/eine Regression zu den Daten. Die Optimierung selbst läuft einfach darüber ab, dass $\\frac{dSSE}{dw}=0$ bestimmt wird. Das optimale Gewicht ist dann gegeben als\n",
    "\\begin{equation}\n",
    "    \\mathbf{w}^* = (\\mathbf{X}^T \\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{y}.\n",
    "\\end{equation}\n",
    "Hierbei ist $\\mathbf{X}=\\begin{pmatrix}\n",
    "    \\mathbf{1} & x_1\\\\\n",
    "    . & .\\\\\n",
    "    . & .\\\\\n",
    "    . & .\\\\\n",
    "    \\mathbf{1} & x_N\n",
    "    \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7511e6-4154-464b-ac41-bde9d0d37b98",
   "metadata": {},
   "source": [
    "## Verallgemeinern\n",
    "\n",
    "Man kann auch nicht lineare Daten genauso fitten. Dazu definiert man sich eine \"Feature Funktion\" $\\phi (x)$. Mit der wird eine Basistransformation ausgeführt, damit die Daten in dieser neuen Basis linear \"aussehen\" und mittels linearer Regression gefittet werden können. Dadurch verändert sich nur die optimale Lösung zu\n",
    "\\begin{equation}\n",
    "    \\mathbf{w}^* = (\\phi(\\mathbf{X})^T \\phi(\\mathbf{X}))^{-1} \\phi(\\mathbf{X})^T \\mathbf{y}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73783403-7db6-4269-a961-6acfe346015e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
